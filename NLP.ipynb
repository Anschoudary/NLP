{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDOlMIs6ZO+TxsDPbmNGeb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anschoudary/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## What is Natural Language Processing (NLP)\n",
        "\n",
        "NLP is a field of Artificial intelligence (AI) that focuses on enabling computers to understand, interpret, and generate human language. It combines principles from linguistics, computer science, and AI to bridge the gap between human communication and machine understanding.\n",
        "Types of NLP\n",
        "\n",
        "    Text Classification: Categorizing text into predefined categories (e.g., spam detection, sentiment analysis).\n",
        "    Machine Translation: Converting text from one language to another.\n",
        "    Question Answering: Extracting answers from text based on given questions.\n",
        "    Text Summarization: Generating concise summaries of longer texts.\n",
        "    Named Entity Recognition (NER): Identifying and classifying named entities (e.g., person, location, organization).\n",
        "\n",
        "### Examples of NLP Applications\n",
        "\n",
        "    Chatbots: Simulating human conversation for customer service or information retrieval.\n",
        "    Virtual Assistants: Performing tasks based on voice commands (e.g., Siri, Alexa).\n",
        "    Search Engines: Understanding user queries to retrieve relevant information.\n",
        "    Social Media Monitoring: Analyzing social media data for insights and trends.\n"
      ],
      "metadata": {
        "id": "EdWPsvipTeFB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se8D7xe8SGRk",
        "outputId": "0fda2b1c-e2c5-4b9d-d67d-936487fe040e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H70Jsv6VTy3L",
        "outputId": "23ccff0d-900e-4b6c-f396-c1793e28464c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization: Splitting text into individual words or sentences.\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"\"\"We don’t regularly think about the intricacies of our own languages.\n",
        "          It’s an intuitive behavior used to convey information and meaning with semantic cues such as words,\n",
        "          signs, or images. It’s been said that language is easier to learn and comes more naturally in adolescence\n",
        "          because it’s a repeatable, trained behavior—much like walking. And language doesn’t follow a strict set\n",
        "          of rules, with so many exceptions like “I before E except after C.” What comes naturally to humans, however,\n",
        "          is exceedingly difficult for computers with the amount of unstructured data, lack of formal rules, and absence\n",
        "          of real-world context or intent. That’s why machine learning and artificial intelligence (AI) are gaining\n",
        "          attention and momentum, with greater human dependency on computing systems to communicate and perform tasks.\n",
        "          And as AI and augmented analytics get more sophisticated, so will Natural Language Processing (NLP).\n",
        "          While the terms AI and NLP might conjure images of futuristic robots, there are already basic examples of\n",
        "          NLP at work in our daily lives. Here are a few prominent examples.\"\"\"\n",
        "tokens = word_tokenize(text)\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "id": "6HYcu7xkT8tl"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop Word Removal: Filtering out common words like \"the\", \"a\", \"is\", etc.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMvD5QV7UKz8",
        "outputId": "1f98c396-e715-46a7-a0bb-3f0551586009"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['’', 'regularly', 'think', 'intricacies', 'languages', '.', '’', 'intuitive', 'behavior', 'used', 'convey', 'information', 'meaning', 'semantic', 'cues', 'words', ',', 'signs', ',', 'images', '.', '’', 'said', 'language', 'easier', 'learn', 'comes', 'naturally', 'adolescence', '’', 'repeatable', ',', 'trained', 'behavior—much', 'like', 'walking', '.', 'language', '’', 'follow', 'strict', 'set', 'rules', ',', 'many', 'exceptions', 'like', '“', 'E', 'except', 'C.', '”', 'comes', 'naturally', 'humans', ',', 'however', ',', 'exceedingly', 'difficult', 'computers', 'amount', 'unstructured', 'data', ',', 'lack', 'formal', 'rules', ',', 'absence', 'real-world', 'context', 'intent', '.', '’', 'machine', 'learning', 'artificial', 'intelligence', '(', 'AI', ')', 'gaining', 'attention', 'momentum', ',', 'greater', 'human', 'dependency', 'computing', 'systems', 'communicate', 'perform', 'tasks', '.', 'AI', 'augmented', 'analytics', 'get', 'sophisticated', ',', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', '.', 'terms', 'AI', 'NLP', 'might', 'conjure', 'images', 'futuristic', 'robots', ',', 'already', 'basic', 'examples', 'NLP', 'work', 'daily', 'lives', '.', 'prominent', 'examples', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming: Reducing words to their base form (e.g., \"running\" to \"run\").\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIUrgGyuUR3R",
        "outputId": "910f5924-fd70-4e72-d10c-a3d631b8e0ab"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['’', 'regularli', 'think', 'intricaci', 'languag', '.', '’', 'intuit', 'behavior', 'use', 'convey', 'inform', 'mean', 'semant', 'cue', 'word', ',', 'sign', ',', 'imag', '.', '’', 'said', 'languag', 'easier', 'learn', 'come', 'natur', 'adolesc', '’', 'repeat', ',', 'train', 'behavior—much', 'like', 'walk', '.', 'languag', '’', 'follow', 'strict', 'set', 'rule', ',', 'mani', 'except', 'like', '“', 'e', 'except', 'c.', '”', 'come', 'natur', 'human', ',', 'howev', ',', 'exceedingli', 'difficult', 'comput', 'amount', 'unstructur', 'data', ',', 'lack', 'formal', 'rule', ',', 'absenc', 'real-world', 'context', 'intent', '.', '’', 'machin', 'learn', 'artifici', 'intellig', '(', 'ai', ')', 'gain', 'attent', 'momentum', ',', 'greater', 'human', 'depend', 'comput', 'system', 'commun', 'perform', 'task', '.', 'ai', 'augment', 'analyt', 'get', 'sophist', ',', 'natur', 'languag', 'process', '(', 'nlp', ')', '.', 'term', 'ai', 'nlp', 'might', 'conjur', 'imag', 'futurist', 'robot', ',', 'alreadi', 'basic', 'exampl', 'nlp', 'work', 'daili', 'live', '.', 'promin', 'exampl', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization: Converting words to their canonical form (e.g., \"better\" to \"good\").\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeZLtHN0UgGY",
        "outputId": "c80f9b84-690e-47f4-bb34-1c3d991c4988"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['’', 'regularli', 'think', 'intricaci', 'languag', '.', '’', 'intuit', 'behavior', 'use', 'convey', 'inform', 'mean', 'semant', 'cue', 'word', ',', 'sign', ',', 'imag', '.', '’', 'said', 'languag', 'easier', 'learn', 'come', 'natur', 'adolesc', '’', 'repeat', ',', 'train', 'behavior—much', 'like', 'walk', '.', 'languag', '’', 'follow', 'strict', 'set', 'rule', ',', 'mani', 'except', 'like', '“', 'e', 'except', 'c.', '”', 'come', 'natur', 'human', ',', 'howev', ',', 'exceedingli', 'difficult', 'comput', 'amount', 'unstructur', 'data', ',', 'lack', 'formal', 'rule', ',', 'absenc', 'real-world', 'context', 'intent', '.', '’', 'machin', 'learn', 'artifici', 'intellig', '(', 'ai', ')', 'gain', 'attent', 'momentum', ',', 'greater', 'human', 'depend', 'comput', 'system', 'commun', 'perform', 'task', '.', 'ai', 'augment', 'analyt', 'get', 'sophist', ',', 'natur', 'languag', 'process', '(', 'nlp', ')', '.', 'term', 'ai', 'nlp', 'might', 'conjur', 'imag', 'futurist', 'robot', ',', 'alreadi', 'basic', 'exampl', 'nlp', 'work', 'daili', 'live', '.', 'promin', 'exampl', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part-of-Speech (POS) Tagging: Assigning grammatical tags to words (e.g., noun, verb, adjective).\n",
        "\n",
        "pos_tags = nltk.pos_tag(lemmatized_words)\n",
        "print(\"POS Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g-P1Z-WU-8r",
        "outputId": "6784c3ff-0269-43a2-8c9c-abdbc47dda58"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('’', 'JJ'), ('regularli', 'NN'), ('think', 'VBP'), ('intricaci', 'JJ'), ('languag', 'NN'), ('.', '.'), ('’', 'CC'), ('intuit', 'NN'), ('behavior', 'NN'), ('use', 'NN'), ('convey', 'JJ'), ('inform', 'NN'), ('mean', 'NN'), ('semant', 'NN'), ('cue', 'NN'), ('word', 'NN'), (',', ','), ('sign', 'NN'), (',', ','), ('imag', 'NN'), ('.', '.'), ('’', 'NN'), ('said', 'VBD'), ('languag', 'RB'), ('easier', 'JJR'), ('learn', 'JJ'), ('come', 'VBP'), ('natur', 'JJ'), ('adolesc', 'NN'), ('’', 'NNP'), ('repeat', 'NN'), (',', ','), ('train', 'VBP'), ('behavior—much', 'JJ'), ('like', 'IN'), ('walk', 'NN'), ('.', '.'), ('languag', 'CC'), ('’', 'JJ'), ('follow', 'JJ'), ('strict', 'NN'), ('set', 'NN'), ('rule', 'NN'), (',', ','), ('mani', 'RB'), ('except', 'IN'), ('like', 'IN'), ('“', 'NNP'), ('e', 'FW'), ('except', 'IN'), ('c.', 'NN'), ('”', 'NNP'), ('come', 'VBP'), ('natur', 'JJ'), ('human', 'JJ'), (',', ','), ('howev', 'NN'), (',', ','), ('exceedingli', 'VBZ'), ('difficult', 'JJ'), ('comput', 'NN'), ('amount', 'NN'), ('unstructur', 'JJ'), ('data', 'NNS'), (',', ','), ('lack', 'NN'), ('formal', 'JJ'), ('rule', 'NN'), (',', ','), ('absenc', 'RB'), ('real-world', 'JJ'), ('context', 'JJ'), ('intent', 'NN'), ('.', '.'), ('’', 'CC'), ('machin', 'JJ'), ('learn', 'NN'), ('artifici', 'NN'), ('intellig', 'NN'), ('(', '('), ('ai', 'NN'), (')', ')'), ('gain', 'NN'), ('attent', 'JJ'), ('momentum', 'NN'), (',', ','), ('greater', 'JJR'), ('human', 'JJ'), ('depend', 'NN'), ('comput', 'NN'), ('system', 'NN'), ('commun', 'JJ'), ('perform', 'NN'), ('task', 'NN'), ('.', '.'), ('ai', 'JJ'), ('augment', 'NN'), ('analyt', 'NN'), ('get', 'VBP'), ('sophist', 'NN'), (',', ','), ('natur', 'JJ'), ('languag', 'NN'), ('process', 'NN'), ('(', '('), ('nlp', 'NN'), (')', ')'), ('.', '.'), ('term', 'NN'), ('ai', 'VBP'), ('nlp', 'NN'), ('might', 'MD'), ('conjur', 'VB'), ('imag', 'JJ'), ('futurist', 'NN'), ('robot', 'NN'), (',', ','), ('alreadi', 'JJ'), ('basic', 'JJ'), ('exampl', 'NN'), ('nlp', 'JJ'), ('work', 'NN'), ('daili', 'RB'), ('live', 'JJ'), ('.', '.'), ('promin', 'NN'), ('exampl', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings\n",
        "\n",
        "This code snippet demonstrates how to train a Word2Vec model using the gensim library. You provide a list of sentences as input, and the model learns word embeddings based on the context of words in the sentences."
      ],
      "metadata": {
        "id": "HufkHJ7gVpex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1IP7w-IVJbh",
        "outputId": "42ced9d5-ccad-438a-f129-4f2470fde2a5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample corpus\n",
        "sentences = [[\"this\", \"is\", \"a\", \"sentence\"], [\"another\", \"sentence\"], [\"yet\", \"another\", \"one\"]]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Get word embedding for \"sentence\"\n",
        "word_embedding = model.wv['sentence']\n",
        "print(word_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69zoVEKoVmcI",
        "outputId": "140dde34-e959-421f-a4e4-a29a9ef1aa88"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
            "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
            " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
            " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
            "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
            "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
            "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
            " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
            " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
            "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
            "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
            " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
            "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
            " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
            "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
            " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
            "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
            "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
            "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
            " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
            " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
            "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
            "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
            "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
            "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagging\n",
        "\n",
        "This example demonstrates how to use the BaggingClassifier from sklearn.ensemble with a DecisionTreeClassifier as the base estimator. You can replace the sample data with your own dataset and experiment with different base estimators and parameters.\n"
      ],
      "metadata": {
        "id": "_xR1T_QMWCxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Sample data (replace with your own dataset)\n",
        "X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
        "y = [0, 1, 0, 1]\n",
        "\n",
        "# Create a decision tree classifier\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "\n",
        "# Create a bagging classifier with 10 estimators\n",
        "bagging_model = BaggingClassifier(base_estimator=base_estimator, n_estimators=10)\n",
        "\n",
        "# Fit the bagging model to the data\n",
        "bagging_model.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = bagging_model.predict([[3, 3]])\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLvkQjlYV4bO",
        "outputId": "bde12b61-7a53-44b0-93cc-a36f6989043a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization\n",
        "\n",
        "This code demonstrates how to use the TfidfVectorizer from sklearn.feature_extraction.text to convert a corpus of text documents into a TF-IDF matrix. Each row in the matrix represents a document, and each column represents a unique word in the corpus. The values in the matrix represent the TF-IDF score of each word in each document."
      ],
      "metadata": {
        "id": "c_dGgqh6WiS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the corpus\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# Transform the corpus into a TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.transform(corpus)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tfidf_matrix.toarray())\n",
        "\n",
        "# Print the feature names\n",
        "print(feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toobgjqXWYLf",
        "outputId": "16605aad-4f5b-4cee-fa86-217a55d3b045"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i8odherrXyev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}